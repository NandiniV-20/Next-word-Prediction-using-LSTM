{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee68f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.7.0\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: https://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD 3-Clause License\n",
      "\n",
      " Copyright (c) 2007-2024 The scikit-learn developers.\n",
      " All rights reserved.\n",
      "\n",
      " Redistribution and use in source and binary forms, with or without\n",
      " modification, are permitted provided that the following conditions are met:\n",
      "\n",
      " * Redistributions of source code must retain the above copyright notice, this\n",
      "   list of conditions and the following disclaimer.\n",
      "\n",
      " * Redistributions in binary form must reproduce the above copyright notice,\n",
      "   this list of conditions and the following disclaimer in the documentation\n",
      "   and/or other materials provided with the distribution.\n",
      "\n",
      " * Neither the name of the copyright holder nor the names of its\n",
      "   contributors may be used to endorse or promote products derived from\n",
      "   this software without specific prior written permission.\n",
      "\n",
      " THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      " AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      " IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      " DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      " FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      " DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      " SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      " CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      " OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      " OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      " ----\n",
      "\n",
      " This binary distribution of scikit-learn also bundles the following software:\n",
      "\n",
      " ----\n",
      "\n",
      " Name: Microsoft Visual C++ Runtime Files\n",
      " Files: sklearn\\.libs\\*.dll\n",
      " Availability: https://learn.microsoft.com/en-us/visualstudio/releases/2015/2015-redistribution-vs\n",
      "\n",
      " Subject to the License Terms for the software, you may copy and distribute with your\n",
      " program any of the files within the followng folder and its subfolders except as noted\n",
      " below. You may not modify these files.\n",
      "\n",
      " C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\n",
      "\n",
      " You may not distribute the contents of the following folders:\n",
      "\n",
      " C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\debug_nonredist\n",
      " C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\onecore\\debug_nonredist\n",
      "\n",
      " Subject to the License Terms for the software, you may copy and distribute the following\n",
      " files with your program in your programâ€™s application local folder or by deploying them\n",
      " into the Global Assembly Cache (GAC):\n",
      "\n",
      " VC\\atlmfc\\lib\\mfcmifc80.dll\n",
      " VC\\atlmfc\\lib\\amd64\\mfcmifc80.dll\n",
      "\n",
      "Location: C:\\Users\\nandi\\anaconda3\\envs\\lstm_env\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: nltk, pandas, tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow nltk pandas scikit-learn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72890e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\nandi\\anaconda3\\envs\\lstm_env\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nandi\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "598c1e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\nandi\\anaconda3\\envs\\lstm_env\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nandi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nandi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.3.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faee6029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\nandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e389cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a602c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nandi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nandi\\anaconda3\\envs\\lstm_env\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nandi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nandi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (4.11.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.73.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
      "  Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.14.0-cp311-cp311-win_amd64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2025.7.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nandi\\anaconda3\\envs\\lstm_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.16.0-cp311-cp311-win_amd64.whl.metadata (31 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nandi\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
      "Using cached grpcio-1.73.1-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl (209 kB)\n",
      "Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached certifi-2025.7.9-py3-none-any.whl (159 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp311-cp311-win_amd64.whl (2.9 MB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.16.0-cp311-cp311-win_amd64.whl (314 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset_normalizer, certifi, astunparse, absl-py, werkzeug, requests, ml-dtypes, markdown-it-py, h5py, tensorboard, rich, keras, tensorflow\n",
      "\n",
      "   - --------------------------------------  1/32 [libclang]\n",
      "   - --------------------------------------  1/32 [libclang]\n",
      "   - --------------------------------------  1/32 [libclang]\n",
      "   - --------------------------------------  1/32 [libclang]\n",
      "   --- ------------------------------------  3/32 [wrapt]\n",
      "   ----- ----------------------------------  4/32 [urllib3]\n",
      "   ---------- -----------------------------  8/32 [protobuf]\n",
      "   ---------- -----------------------------  8/32 [protobuf]\n",
      "   ----------- ----------------------------  9/32 [optree]\n",
      "  Attempting uninstall: numpy\n",
      "   ----------- ----------------------------  9/32 [optree]\n",
      "    Found existing installation: numpy 2.3.1\n",
      "   ----------- ----------------------------  9/32 [optree]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "    Uninstalling numpy-2.3.1:\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "      Successfully uninstalled numpy-2.3.1\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ------------- -------------------------- 11/32 [numpy]\n",
      "   ---------------- ----------------------- 13/32 [MarkupSafe]\n",
      "   ----------------- ---------------------- 14/32 [markdown]\n",
      "   -------------------- ------------------- 16/32 [grpcio]\n",
      "   -------------------- ------------------- 16/32 [grpcio]\n",
      "   --------------------- ------------------ 17/32 [google-pasta]\n",
      "   ----------------------- ---------------- 19/32 [charset_normalizer]\n",
      "   --------------------------- ------------ 22/32 [absl-py]\n",
      "   ---------------------------- ----------- 23/32 [werkzeug]\n",
      "   ---------------------------- ----------- 23/32 [werkzeug]\n",
      "   -------------------------------- ------- 26/32 [markdown-it-py]\n",
      "   -------------------------------- ------- 26/32 [markdown-it-py]\n",
      "   --------------------------------- ------ 27/32 [h5py]\n",
      "   --------------------------------- ------ 27/32 [h5py]\n",
      "   ----------------------------------- ---- 28/32 [tensorboard]\n",
      "   ----------------------------------- ---- 28/32 [tensorboard]\n",
      "   ----------------------------------- ---- 28/32 [tensorboard]\n",
      "   ----------------------------------- ---- 28/32 [tensorboard]\n",
      "   ----------------------------------- ---- 28/32 [tensorboard]\n",
      "   ----------------------------------- ---- 28/32 [tensorboard]\n",
      "   ----------------------------------- ---- 28/32 [tensorboard]\n",
      "   ------------------------------------ --- 29/32 [rich]\n",
      "   ------------------------------------ --- 29/32 [rich]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   ------------------------------------- -- 30/32 [keras]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   -------------------------------------- - 31/32 [tensorflow]\n",
      "   ---------------------------------------- 32/32 [tensorflow]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 certifi-2025.7.9 charset_normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.14.0 idna-3.10 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 requests-2.32.4 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 urllib3-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\nandi\\anaconda3\\envs\\lstm_env\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\nandi\\anaconda3\\envs\\lstm_env\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d1b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a72d852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Using cached numpy-2.3.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.0-cp311-cp311-win_amd64.whl (10.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached numpy-2.3.1-cp311-cp311-win_amd64.whl (13.0 MB)\n",
      "Using cached scipy-1.16.0-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   ---------------------------------------- 5/5 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 numpy-2.3.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cc4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d11add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('hamlet.txt','r') as file:\n",
    "    text=file.read().lower()\n",
    "    \n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af815522",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsequences=[]\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        inputsequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cbdf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len=max([len(x) for x in inputsequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdad81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences=np.array(pad_sequences(inputsequences,maxlen=max_sequence_len,padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a026874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x,y=input_sequences[:,:-1],input_sequences[:,-1]\n",
    "y=tf.keras.utils.to_categorical(y,num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f2dad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8585319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nandi\\anaconda3\\envs\\lstm_env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d765b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.0344 - loss: 7.1771 - val_accuracy: 0.0326 - val_loss: 6.7255\n",
      "Epoch 2/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.0370 - loss: 6.4716 - val_accuracy: 0.0416 - val_loss: 6.8196\n",
      "Epoch 3/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.0419 - loss: 6.3172 - val_accuracy: 0.0511 - val_loss: 6.8412\n",
      "Epoch 4/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.0511 - loss: 6.1600 - val_accuracy: 0.0523 - val_loss: 6.8752\n",
      "Epoch 5/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.0545 - loss: 6.0407 - val_accuracy: 0.0525 - val_loss: 6.9150\n",
      "Epoch 6/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.0596 - loss: 5.9011 - val_accuracy: 0.0624 - val_loss: 6.9611\n",
      "Epoch 7/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 38ms/step - accuracy: 0.0696 - loss: 5.7385 - val_accuracy: 0.0678 - val_loss: 7.0141\n",
      "Epoch 8/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.0765 - loss: 5.6189 - val_accuracy: 0.0705 - val_loss: 7.0508\n",
      "Epoch 9/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.0828 - loss: 5.4701 - val_accuracy: 0.0705 - val_loss: 7.1330\n",
      "Epoch 10/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.0928 - loss: 5.3044 - val_accuracy: 0.0736 - val_loss: 7.2566\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train,epochs=10,validation_data=(X_test,Y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edf8d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model,tokenizer,text,max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list)<=max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]\n",
    "    token_list = pad_sequences([token_list],maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted=model.predict(token_list,verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted,axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index==predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a4a676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:To be or not to be\n",
      "Next word prediction:not\n"
     ]
    }
   ],
   "source": [
    "input_text=\"To be or not to be\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len-1)\n",
    "print(f\"Next word prediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f9274b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"next_word_lstm.h5\")\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "724b1007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:Barn. Well, goodnight. If you do meet Horatio and Marcellus, the Riuals of my Watch, bid them make\n",
      "Next word prediction:him\n"
     ]
    }
   ],
   "source": [
    "input_text=\"Barn. Well, goodnight. If you do meet Horatio and Marcellus, the Riuals of my Watch, bid them make\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len-1)\n",
    "print(f\"Next word prediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd07146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505fcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e5ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
